{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliografia usada en este documento:\n",
    "\n",
    "* https://pythonhosted.org/PyQt-Fit/NonParam_tut.html\n",
    "* Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "Una regresión no paramétrica es una forma de análisis de la regresión en el que el predictor no tiene una forma predeterminada, sino que se construye de acuerdo a la información derivada de los datos. La regresión no paramétrica requiere tamaños de muestra más grandes que los de una regresión sobre la base de modelos paramétricos porque los datos deben suministrar la estructura del modelo, así como las estimaciones del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos de regresiones no paramétricas\n",
    "\n",
    "#### a) Regresión Kernel\n",
    "La regresión Kernel estima la variable dependiente continua a partir de un conjunto limitado de puntos de datos por <code style=\"font-family:Arial;background:#d4d9de;color:black;font-size:1.0em \">convolución</code> de las ubicaciones de los puntos de datos con una función kernel.\n",
    "\n",
    "**Convolución**: En matemáticas, y en particular análisis funcional, una convolución es un operador matemático que transforma dos funciones f y g en una tercera función que en cierto sentido representa la magnitud en la que se superponen f y una versión trasladada e invertida de g. Una convolución es un tipo muy general de media móvil:\n",
    "\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "    <tr>\n",
    "        <td><img src=\"IMAGES/Convolucion_2cuadros.gif\" width=310 height=310></td>\n",
    "        <td><img src=\"IMAGES/Convolucion_cuadro_exp.gif\" width=220 height=220></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"2\"><center>Ejemplos de convolución de funciones</center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### b) Regresión no paramétrica multiplicativa\n",
    "La regresión no paramétrica multiplicativa (NPMR) es una forma de regresión no paramétrica basada en una estimación multiplicativa del kernel. Al igual que otros métodos de regresión, el objetivo es estimar una respuesta (variable dependiente) sobre la base de uno o más predictores (variables independientes). La NPMR puede ser una buena opción para un método de regresión si se cumplen las siguientes condiciones:\n",
    "\n",
    "* La forma de la superficie de respuesta es desconocida.\n",
    "* Los predictores son propensos a interactuar en la producción de la respuesta, en otras palabras, la forma de la respuesta a un predictor es probable que dependa de otros predictores.\n",
    "* La respuesta es o bien una variable cuantitativa o binaria (0/1).\n",
    "Esta es una técnica de suavizado que se puede cruzar validado y aplicado de una manera predecible.\n",
    "\n",
    "#### c) Árboles de regresión\n",
    "Véase también: Árbol de decisión\n",
    "Los algoritmos en los árboles de decisión se pueden aplicar para aprender a predecir una variable dependiente a partir de datos.\n",
    "A pesar de que la formulación CART original aplica sólo para la predicción de los datos univariados, el marco puede ser usado para predecir los datos multivariantes, incluyendo series de tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimadores de densidad\n",
    "\n",
    "### 1. Histograma\n",
    "Sus características principales son: <br></br>\n",
    "\n",
    "**a) El resultado es dependiente en el origen x0.**<br></br>\n",
    "Al construir un histograma, el investigador debe escoger la posición en la cual colocar el origen de\n",
    "los intervalos. Este elemento de elección en la construcción del histograma puede interactuar con otras elecciones (como el número y amplitud de los intervalos) para producir resultados engañosos (Tarter y Kronmal, 1976): Histogramas con varios origenes los cuales producen impresiones diferentes de la distribución de un sólo lote de datos (por ejemplo, distribuciones unimodales o bimodales dependiendo de la posición del origen).<br></br>\n",
    "\n",
    "**b) El resultado depende de la amplitud y número de intervalos**<br></br>\n",
    "Tarter y Kronmal (1976) señalaron que el número de intervalos debería determinarse con base en alguna función del tamaño de la muestra. Para lotes grandes, un gran número de intervalos debería dar una representación suave de la función de densidad desconocida. Al utilizar unos pocos intervalos, sin embargo, resultará en la pérdida de cualquier detalle de la distribución subyacente. El considerar muy pocos intervalos producirá una imagen sin características.\n",
    "Frecuentemente, el número de intervalos y su amplitud se determinan arbitrariamente a pesar de su importancia, la cual radica en que **la amplitud de intervalo determina el grado de suavidad del histograma resultante**.\n",
    "\n",
    "La amplitud fija de intervalo resulta en una representación desproporcionada de la densidad en el centro y las de las colas de la distribución.<br></br>\n",
    "\n",
    "La Figura 3.1 presenta cinco histogramas (con fracción como escala para el eje de las ordenadas) con la misma amplitud del intervalo (h = 10) pero diferente origen, cualquiera de los cuales es una estimación válida de la densidad (después de la transformación de escala correspondiente).\n",
    "\n",
    "<img src=\"IMAGES/Histogramas_desplazados.png\" width=\"550\" height=\"800\" >\n",
    "\n",
    "Algunos de estos histogramas son unimodales, otros bimodales e inclusive trimodales. En todos ellos parece existir una moda alrededor de 80; en otros se despliegan modas secundarias alrededor de 50 o 100. Podemos escoger alguno de ellos para representar la distribución de los datos, pero la elección sería arbitraria. Para eliminar el efecto de la elección del origen, Scott (1985b) sugirió un ingenioso artificio: en lugar de escoger entre varios histogramas, Scott propuso promediar varios histogramas para obtener el <code style=\"font-family: Arial; background:#d4d9de;color:black;font-size:1.0em \">histograma desplazado promedio (HDP)</code>.\n",
    "\n",
    "El método del HDP es un caso especial del concepto más general desarrollado por Härdle y Scott (1988): <code style=\"font-family: Arial; background:#d4d9de;color:black;font-size:1.0em \">promedio ponderado de puntos redondeados ó PPPR</code>.\n",
    "\n",
    "El PPPR está basado en los intervalos más angostos, los cuales están definidos por h (el parámetro de suavización), la amplitud de banda y un nuevo parámetro **M**, que es el número de histogramas desplazados para promediar.\n",
    "\n",
    "En resumen, <code style=\"font-family: Arial; background:#d4d9de;color:black;font-size:1.0em \">el histograma se define al especificar dos parámetros: el origen (xo) y la amplitud (h) de los intervalos</code>. No es adecuado para estimar la distribución de la población subyacente, ni para propósitos inferenciales ni para comparar la distribución de varias poblaciones.\n",
    "A pesar de sus inconvenientes, el valor del histograma como una importante y útil herramienta estadística es innegable.\n",
    "\n",
    "### 2. Poligonos de frecuencia\n",
    "Como se afirmó anteriormente, las discontinuidades del histograma limitan su utilidad como un\n",
    "estimador de densidad. El polígono de frecuencia (PF) es un estimador de densidad continuo\n",
    "derivado por la interpolación lineal de los centros de clase del histograma. El PF conecta dos valores adyacentes del\n",
    "histograma entre los centros de clase.<br></br>\n",
    "La aleatoriedad en el polígono de frecuencia deriva enteramente de los intervalos del histograma.\n",
    "\n",
    "Del trabajo de Scott(1985a) puede afirmarse que el PF en contraste con el histograma:\n",
    "- aproxima mejor a densidades continuas por medio de interpolación lineal de intervalos más amplios.\n",
    "- es menos eficiente cuando la densidad subyacente es discontinua.\n",
    "- es más sensitiva con respecto a errores en la elección de la amplitud de intervalo.\n",
    "- es mas eficiente para los datos en relación al histograma al crecer el tamaño de muestra.\n",
    "\n",
    "La Figura 1.3 muestra el PF con amplitud de intervalo de 8 como en el histograma\n",
    "de la Figura 1.2 pero con origen en 216. El PF de la Figura 1.4 tiene una amplitud de intervalo de 16\n",
    "con un origen en 208.\n",
    "\n",
    "<img src=\"IMAGES/Histograma_Pol_Freq.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Estimadores de densidad por kernel\n",
    "La estimación de densidad kernel (KDE) es menos usada que el histograma, pero puede ser una útil herramienta para mostrar la distribución de una variable. Al igual que aquel, los KDE codifican la densidad de observaciones en un eje mostrando una altura proporcional en el otro eje, pero los KDE pueden ser dotados de propiedades como la suavidad o continuidad.\n",
    "\n",
    "Para construir el KDE se considera una función no negativa -el kernel- y un parámetro de suavizado denominado bandwidth. Cuando el kernel es una función gaussiana, cada observación es sustituida por una curva de este tipo centrada en dicho valor. A continuación, se suman las curvas para obtener el valor de la densidad en cada punto. Por último, la curva resultante se normaliza para que el área bajo ella sea igual a 1.\n",
    "\n",
    "#### Estimadores simples\n",
    "* Trazas de densidad\n",
    "* Estimador de densidad simple\n",
    "\n",
    "#### Estimadores más sofisticados de densidad por kernel\n",
    "Para resolver el problema de la discontinuidad causada por las esquinas cuadradas de la función\n",
    "ponderal rectangular que utiliza el estimador simple presentado con anterioridad (y por otras\n",
    "razones técnicas discutidas por Silverman, 1986), es conveniente considerar la generalización de\n",
    "este estimador de densidad simple. Esto se consigue reemplazando la función ponderal S(•) por otra\n",
    "produciendo figuras redondeadas en lugar de rectángulos. Para indicar esta distinción la nueva\n",
    "función ponderal se anota como función por kernel K(•)\n",
    "\n",
    "Algunas funciones por kernel se incluyen en el Cuadro 2.1, adaptado de Silverman (1986): \n",
    "<code style=\"font-family: Arial; background:#d4d9de;color:black;font-size:1.0em \">Epanechnikov</code>, \n",
    "<code style=\"font-family: Arial; background:#d4d9de;color:black;font-size:1.0em \">Biponderado</code>, \n",
    "<code style=\"font-family: Arial; background:#d4d9de;color:black;font-size:1.0em \">Triangular</code>, \n",
    "<code style=\"font-family: Arial; background:#d4d9de;color:black;font-size:1.0em \">Gaussiano</code>, \n",
    "<code style=\"font-family: Arial; background:#d4d9de;color:black;font-size:1.0em \">Rectangular</code>.\n",
    "\n",
    "Como con otros suavizadores, para evaluar el desempeño de estos kerneles es necesario considerar el compromiso entre la varianza y el sesgo. La suma de la varianza y sesgo integrados es el **error cuadrado integrado (total) medio (ECIM)**. <u>Una buena función kernel deberá minimizar el sesgo por la asignación de un peso mayor a las observaciones cercanas al valor de x en el cual se estima la densidad</u>.\n",
    "\n",
    "#### Estimadores de densidad por kernel de amplitud variable\n",
    "\n",
    "Todos los estimadores de la densidad por kernel descritos con anterioridad utilizan amplitud fija de ventana. Esta característica provoca que las estimaciones sean vulnerables al ruido en las colas o en cualquier intervalo con cuenta baja de observaciones en la distribución.<br></br>\n",
    "<code style=\"font-family: Arial; background:#d4d9de;color:black;font-size:1.0em \">La idea general es el ajustar la amplitud de ventana de tal forma que sea más angosta a densidades altas y más amplia donde existen bajas densidades</code>. El resultado de este procedimiento de amplitud variable es el <u>retener detalle donde las observaciones se concentran y eliminar fluctuaciones ruidosas donde los datos escasean</u>.\n",
    "\n",
    "#### La elección de la amplitud de ventana\n",
    "La elección de la amplitud de ventana en las estimaciones por kernel es equivalente a la selección de amplitud de intervalo en histogramas. Esta elección determina las características cualitativas de la densidad por kernel.<br></br>\n",
    "Un enfoque, sugerido por Tarter y Kronmal (1976) es variar h hasta que\n",
    "resulte una figura satisfactoria (por lo general suave). Este procedimiento depende de la evaluación\n",
    "subjetiva del investigador. La teoría estadística proporciona algunos lineamientos para la elección de la amplitud. Desafortunadamente, como en el caso del histograma, por lo general no es posible optimizar la amplitud de ventana sin el conocimiento previo de la forma verdadera de la densidad.\n",
    "\n",
    "Para el calculo de una aproximacion inicial, puede emplearse a la distribución gaussiana como un estándar de referencia y entonces, aplicando un kernel gaussiano y minimizando al ECIM es posible utilizar las expresiones siguientes (Fox, 1990):\n",
    "\n",
    "<img src=\"IMAGES/kernel_amplitud_ventana.png\">\n",
    "\n",
    "Donde s es la menor de dos estimaciones de parámetro σ de dispersión (escala) de la distribución gaussiana, es decir, la clásica desviación estándar insesgada y la robusta F-pseudosigma (H dispersión/ 1.349) basada en la dispersión de los cuartos (o en inglés conocida también como hinge spread) (Hoaglin, 1983), Fox, 1990). Este ajuste proporciona resistencia a colas potentes y funcionará bien para una amplia gama de densidades, pero, como lo indicó Silverman (1986), tiende a sobresuavizar distribuciones fuertemente sesgadas y multimodales. En este último caso, esta amplitud “óptima” de ventana puede considerarse como un buen punto de partida para afinación posterior.\n",
    "\n",
    "\n",
    "#### Kernel equivalentes\n",
    "Cuando comparamos dos tipos diferentes de kernel que utilizan la misma amplitud de ventana encontramos que los resultados no son los mismos. En general, la causa de esta diferencia es que a pesar de tener el mismo intervalo de soporte, cada uno de los kerneles tienen varianzas diferentes.\n",
    "\n",
    "El cuadro 3.1 (Modificado de Härdle, 1991 y Scott, 1992) da un resumen de tales factores de interconversión entre kerneles populares:\n",
    "\n",
    "<img src=\"IMAGES/Cuadro_conversion_kernel_equivalentes.png\">\n",
    "\n",
    "Al aplicar el factor correspondiente, podemos obtener aproximadamente el mismo grado de suavización con los kerneles triponderado y Gaussiano. Por lo tanto, de acuerdo al cuadro, se puede obtener casi la misma suavización con un kernel Gaussiano si multiplicamos por 2.978 la banda para kernel triponderado.\n",
    "\n",
    "Otro enfoque cercanamente relacionado (más general) es la transformación canónica de banda propuesta por Marron y Nolan (1988).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reglas prácticas para selección de intervalo/banda en estimación univariada de la densidad\n",
    "La elección de la amplitud del intervalo/banda (parámetro de suavización) es uno de los problemas más relevantes de la estimación de la densidad. Como se discutió en los apartados anteriores, existen varios procedimientos para establecer un valor adecuado para este parámetro en histogramas, polígonos de frecuencia (PF), histogramas desplazados promedio (estimadores HDP-PPPR) y estimadores de densidad por kernel (EDK’s). Algunos de estos métodos de selección se enfocan al número “óptimo” de intervalos, mientras que otros producen aproximaciones a la amplitud óptima de intervalo al minimizar alguna medida del error estadístico bajo ciertas condiciones y suposiciones. \n",
    "En esta sección se presentan varias reglas que proporcionan algunos valores de referencia útiles para la elección del parámetro de suavización en el análisis de la densidad por histogramas, polígonos de frecuencia y estimadores de densidad por kernel.\n",
    "\n",
    "**1. Reglas para elección de número de intervalos en histogramas**<br></br>\n",
    "La regla más famosa para determinar el número de intervalos para la estimación de densidad del histograma fue propuesta por Sturges (1926).\n",
    "De acuerdo a la sugerencia de Sturges, el número de intervalos puede calcularse como: <br></br> \n",
    "\n",
    "$ k = 1 + log_{2}(n) $ <br></br>  \n",
    "donde k es el número de intervalos, y n se refiere al número de observaciones.<br></br>\n",
    "Esta regla no es aplicable a datos con distribución asimétrica, no Gaussiana o multimodal.\n",
    "\n",
    "**2. Reglas para elección de amplitud de intervalo en histogramas**<br></br>\n",
    "Scott (1979) derivó una fórmula para calcular la amplitud óptima asintótica resultante en un error cuadrado integrado medio mínimo (ECIM) para histogramas. La fórmula de Scott involucra el conocimiento previo de la verdadera función de densidad, un evento más bien raro en el análisis de datos en la realidad. Por lo tanto, adoptando a la densidad Gaussiana como referencia, el propuso:<br></br>\n",
    "\n",
    "$ \\hat{h} = 3.5\\hat{\\sigma}n^{−1/3} $\n",
    "\n",
    "Una regla más robusta ha sido propuesta por <code style=\"font-family: Arial; background:#d4d9de;color:black;font-size:1.0em \">Freedman y Diaconis (1981a,b)</code>, en la que se reemplaza la estimación de la desviación estándar por un múltiplo del recorrido intercuartílico (RIC). La regla F-D se define como:<br></br>\n",
    "\n",
    "$ \\hat{h} = 2(RIC )n^{−1/3} $\n",
    "\n",
    "En general, las densidades no Gaussianas requieren de más intervalos.\n",
    "\n",
    "\n",
    "**3. Reglas para elección de amplitud de banda en EDK’s**<br></br>\n",
    "Silverman propuso utilizar una distribución estándar como referencia (de forma semejante a como hizo Scott, 1979 para los histogramas). Si se emplea un kernel Gaussiano, la amplitud óptima de banda es estimada por medio de: \n",
    "\n",
    "$ \\hat{h} = 1.06 \\sigma n^{-1/5} $\n",
    "\n",
    "la amplitud de banda resultante de esta ecuacion sobresuaviza datos marcadamente sesgados, es notablemente insensible a la curtosis (usando a distribuciones log-normal, y de familia t como modelos), y sobresuaviza más y más al volverse la distribución más bimodal.\n",
    "Por ello, el mismo sugirió usar una medida robusta de la dispersión tal como el recorrido intercuartílico:<br></br>\n",
    "\n",
    "$ \\hat{h} = 0.79 R n^{−1/5} $\n",
    "\n",
    "Esta expresión da mejores resultados en distribuciones sesgadas y de cola larga pero para el caso bimodal sobresuaviza en forma adicional.\n",
    "\n",
    "\n",
    "**4. Validación cruzada por mínimos cuadrados (VCMC)**<br></br>\n",
    "La validación cruzada (VC) es un procedimiento bien conocido para la elección automática del parámetro de suavización. Existen dos tipos de VC: VC por máxima verosimilitud y VC por mínimos cuadrados (VCMC). Fue sugerida por Rudemo (1982) y Bowman (1984), y está basada en una idea muy simple: se considera al Error Cuadrado Integrado (ECI) como una medida de distancia (d) entre el estimador f^ de la densidad.\n",
    "Un problema de este metedo a la hora de su cálculo es que el número de iteraciones es cuadrático en el número de observaciones, un inconveniente que motiva la búsqueda de otro método más eficiente de cálculo. A este respecto, Silverman (1986) propuso el uso de un algoritmo basado en la transformación rápida de Fourier y Härdle (1991) presenta un algoritmo eficiente para cálculo que requiere un número lineal de iteraciones en el número de observaciones.\n",
    "\n",
    "\n",
    "**5. Validación cruzada sesgada (VCS)**<br></br>\n",
    "Tomando un enfoque diferente, Scott y Terrell (1987) consideraron estimar directamente el error\n",
    "cuadrado integrado medio asintótico (ECIMA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación no paramétrica de la multimodalidad\n",
    "Los investigadores que trabajan con formas complejas de distribución han vuelto su atención en años recientes hacia las técnicas no-paramétricas tales como la estimación de densidad por kernel (Silverman, 1986) donde los componentes individuales mezclados pueden detectarse por la identificación de modas (máximos locales) en la distribución subyacente (Izenman y Sommer,\n",
    "1988).\n",
    "No hay garantía de que una mezcla de densidades unimodales resulten en una densidad multimodal con modas correspondiendo al número y localización de cada componente individual. Sin embargo, en muchas instancias prácticas, la existencia de más de una sola moda sugiere la evidencia de una mezcla. Varias pruebas han sido propuestas para detectar la multimodalidad en una distribución (Hartigan y Hartigan, 1985).\n",
    "\n",
    "En este aprtado se introducirán brevemente dos procedimientos paramétricos para la evaluación de la multimodalidad.\n",
    "\n",
    "a) La prueba “dip” de unimodalidad\n",
    "\n",
    "b) Prueba de Silverman para multimodalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión no paramétrica: estimadores por kernel, ASH-WARP y k-NN\n",
    "La regresión es sin duda el procedimiento estadístico más utilizado. Tiene numerosas variaciones en relación con la naturaleza de los datos analizados: regresión por cuantiles (mediana), regresión robusta, regresión logística, regresión ordinaria por mínimos cuadrados, además de los programas recientemente propuestos para calcular Modelos Lineales Generalizados\n",
    "(denominados glm y glmr, Hilbe, 1993, 1994a; Royston, 1994a).\n",
    "\n",
    "De acuerdo con Silverman (1985), el análisis de regresión proporciona un medio para explorar y presentar relaciones bivariadas, da predicciones y permite encontrar propiedades interesantes de la ecuación resultante. En este contexto, un estimador no paramétrico es deseable debido a que no obliga a que el modelo pertenezca a una clase rígidamente definida, sino que deja\n",
    "que los datos “hablen por sí mismos” en la elección del modelo. En algunos casos, el estimador no paramétrico puede sugerir un modelo paramétrico adecuado (tal como la regresión lineal simple); en otros, puede ser la forma de descubrir una función media muy compleja (no lineal).\n",
    "\n",
    "#### El enfoque tradicional\n",
    "No comento nada\n",
    "\n",
    "#### El estimador Nadaraya-Watson (regresión por kernel)\n",
    "El enfoque no paramétrico para la regresión se relaciona con la estimación de densidad. En la estimación de densidades a un valor dado de x, <u>se consideran los puntos en un pequeño intervalo alrededor de x (ancho de intervalo en histogramas o amplitud de banda h para los estimadores de densidad por kernel)</u>. En la regresión, la variable de respuesta Y es ponderada en un cierto <u>vecindario de x</u>.\n",
    "\n",
    "Utilizando una notación simplificada es posible indicar la forma general para la regresión no paramétrica:\n",
    "\n",
    "\n",
    "$\\hat{m}_{h}(x) = n^{-1} \\sum \\limits _{i=1} ^{n} W_{hi}(x)Y_{i} $\n",
    "\n",
    "Esta expresión, aplicable a la mayor parte de los métodos no paramétricos de regresión, puede entenderse como un promedio ponderado de la variable de respuesta Yi con pesos $ W_{hi}(x) $ que depende del procedimiento y en la distancia entre x y Xi utilizando un parámetro de suavización h.\n",
    "\n",
    "Las aportaciones de Nadaraya (1964) y Watson (1964) nos llevan a la siguente fórmula:\n",
    "\n",
    "$ \\hat{m}_{h}(x) = n^{-1} \\frac{\\sum \\limits _{i=1}^{n} K_{h}(x-X_{i})Y_{i} }  {n^{-1} \\sum \\limits _{j=1}^{n} K_{h}(x-X_{j})  } $\n",
    "\n",
    "- Los pesos dependen de toda la muestra X a través de la estimación de densidad por kernel fh(x)\n",
    "- Las observaciones Yi obtienen mayor peso donde son escasas las correspondientes Xi\n",
    "- Cuando el denominador es cero, entonces el numerador también es cero y la estimación se define como cero\n",
    "- Al h $ \\to $ 0, Xi converge a Yi y la estimación se iguala a una interpolación lineal entre los datos\n",
    "- Al h $ \\to $ ∞, la función ponderada converge a 1 para todos los valores de x, y la estimación converge al valor medio de Y.\n",
    "- Como en la estimación de densidad por kernel el valor de h determina la suavidad de la estimación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilizando el Promedio Ponderado de Puntos Redondeados (PPPR) para calcular la regresión por kernel (Nadaraya-Watson)\n",
    "Es posible definir, en general, la aproximación por PPPR de la estimación Nadaraya-Watson.<br></br>\n",
    "Como sucede en la estimación de densidad por kernel utilizando los HDP o el PPPR, al aumentar el valor de M, la estimación es más cercana a la estimación por kernel.\n",
    "> M: número de histogramas desplazados para promediar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección de la amplitud de banda\n",
    "Para efectuar una estimación de densidad por regresión no paramétrica se requiere escoger un valor para h, el parámetro de suavización. Se debe lograr un buen “compromiso” que evite el obtener una curva que siga a cada uno de los puntos (sobre-ajuste) o por el otro lado para obtener una curva que pierda características importantes de los datos (sobresuavizado).\n",
    "\n",
    "Como en el caso de la estimación de la densidad, se han propuesto varios métodos para escoger la amplitud de banda en la regresión por kernel:\n",
    "+ La forma más simple es probar varios valores y escoger aquél que resulte en una estimación de interés para el analista, esto es, una elección subjetiva.\n",
    "+ Validación cruzada (estimación repetitiva dejando una observación fuera).\n",
    "\n",
    "En la estimación de densidad, el enfoque utilizado es tener una aproximación de la amplitud de banda que minimiza al **error cuadrado integrado medio asintótico (ECIMA)** por medio del cálculo del error de secuencias de amplitudes.<br></br>\n",
    "En la regresión, sin embargo, existen otros estimadores de la discrepancia entre $ \\hat{m}_{h} $ y la curva desconocida descrita por m:\n",
    "- Error Cuadrado Promedio ECP(h)\n",
    "- Error Cuadrado Integrado ECI(h)\n",
    "- Error Cuadrado Condicionado ECC(h)\n",
    "- Error Cuadrado Integrado Medio ECIM(h)<br></br>\n",
    "\n",
    "Cualquiera de estas medidas de distancia puede aplicarse a la estimación de la amplitud de banda óptima puesto que conducen asintóticamente a la misma amplitud de banda (Härdle y Marron, 1986). Entonces, por conveniencia, se sugiere utilizar la distancia más fácil de calcular, o sea el **Error Cuadrado Promedio** (Härdle, 1990, 1991).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otros métodos: estimación de los k-vecinos más cercanos (k-NN)\n",
    "En lugar de considerar un intervalo fijo en la vecindad de x (es decir, una amplitud fija de banda) para calcular un promedio local de observaciones Yi como sucede en la regresión por kernel, es posible considerar un tamaño de muestra fijo con vecindades variables en X. Este enfoque es utilizado en la estimación de regresión de los k vecinos más cercanos ó k-NN por sus siglas en inglés (k Nearest Neighbor), la cual puede definirse como:\n",
    "\n",
    "$ \\hat{m}_{k}(x) = n^{-1} \\sum \\limits _{i=1} ^{n} W_{k i}(x)Y_{i} $\n",
    "\n",
    "En este procedimiento el parámetro de suavización es k, el número de puntos vecinos y como en la regresión por kernel, existe un compromiso entre el sesgo y la varianza. Por lo tanto, incrementando k se obtienen estimaciones más suaves aunque con un sesgo mayor. Existen dos casos extremos:\n",
    "1. cuando k = n, la estimación es igual al promedio de la variable de respuesta\n",
    "2. cuando k = 1, la estimación es una función interpolante que se ajusta a cada uno de los valores de respuesta.\n",
    "\n",
    "Resulta conveniente definir estimaciones k-NN simétricas, esto es, calculando el promedio de observaciones Y en número k/2 a la izquierda y k/2 a la derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimadores no paramétricos adicionales\n",
    "\n",
    "+ **Estimadores locales de nivel**. \n",
    "    Los procedimientos más simples de regresión no paramétrica son versiones locales de estimadores de nivel. En Stata podemos utilizar medianas (estimador de localización) para conectar bandas de puntos de datos (las bandas representan una vecindad\n",
    "    o intervalo en X) al graficar; además, es posible utilizar el comando ksm.ado sin opciones (aparte de la amplitud de banda) para obtener un suavizador basado en promedios móviles o utilizar este programa con la opción de peso (además de la amplitud de banda) para obtener un promedio móvil con ponderación tricúbica.\n",
    "\n",
    "+ **Suavizadores “spline”**. Un “spline” es un suavizador lineal muy relacionado con la regresión por kernel. Por lo tanto, al utilizar el comando graph con la opción “c” para conectar a los puntos y escogiendo un número adecuado de bandas (el parámetro de suavización) es posible obtener en forma gráfica a estos estimadores no paramétricos de la regresión.\n",
    "\n",
    "+ **Suavizadores de regresión local**. Con el comando ksm.ado es posible calcular estimadores de regresión local ponderada y no ponderada (LOWESS) (consultar a Salgado-Ugarte y Shimizu, 1995 para profundizar acerca de algunas mejoras a este comando).\n",
    "\n",
    "+ **Suavizadores no lineales resistentes**. El comando smooth de Stata permite explorar los valores de respuesta por medio de combinaciones diferentes de suavizadores resistentes basados en la mediana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
